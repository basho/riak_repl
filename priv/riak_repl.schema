%%-*- mode: erlang -*-
%% Replication config

%% @doc Path (relative or absolute) to the working directory for the
%% replication process
{mapping, "mdc.data_root", "riak_repl.data_root", [
    {default, "{{repl_data_root}}"}
]}.

%% @doc The cluster manager will listen for connections from remote
%% clusters on this ip and port. Every node runs one cluster manager,
%% but only the cluster manager running on the cluster_leader will
%% service requests. This can change as nodes enter and leave the
%% cluster. The value is a combination of an IP address (**not
%% hostname**) followed by a port number
{mapping, "mdc.cluster_manager", "riak_core.cluster_mgr", [
    {default, {"{{cluster_manager_ip}}", {{cluster_manager_port}} }},
    {datatype, ip}
]}.

%% @doc The hard limit of fullsync workers that will be running on the
%% source side of a cluster across all nodes on that cluster for a
%% fullsync to a sink cluster. This means if one has configured
%% fullsync for two different clusters, both with a
%% max_fssource_cluster of 5, 10 fullsync workers can be in
%% progress. Only affects nodes on the source cluster on which this
%% parameter is defined via the configuration file or command line
{mapping, "mdc.max_fssource_cluster", "riak_repl.max_fssource_cluster", [
    {datatype, integer},
    {default, 5}
]}.

%% @doc Limits the number of fullsync workers that will be running on
%% each individual node in a source cluster. This is a hard limit for
%% all fullsyncs enabled; additional fullsync configurations will not
%% increase the number of fullsync workers allowed to run on any node.
%% Only affects nodes on the source cluster on which this parameter is
%% defined via the configuration file or command line
{mapping, "mdc.max_fssource_node", "riak_repl.max_fssource_node", [
    {datatype, integer},
    {default, 1}
]}.

%% @doc Limits the number of fullsync workers allowed to run on each
%% individual node in a sink cluster. This is a hard limit for all
%% fullsync sources interacting with the sink cluster. Thus, multiple
%% simultaneous source connections to the sink cluster will have to
%% share the sink node's number of maximum connections. Only affects
%% nodes on the sink cluster on which this parameter is defined via
%% the configuration file or command line.
{mapping, "mdc.max_fssink_node", "riak_repl.max_fssink_node", [
    {datatype, integer},
    {default, 1}
]}.

%% @doc Whether to initiate a fullsync on initial connection from the
%% secondary cluster
{mapping, "mdc.fullsync_on_connect", "riak_repl.fullsync_on_connect", [
    {datatype, {enum, [true, false]}},
    {default, true}
]}.

%% @doc a single integer value representing the duration to wait in
%% minutes between fullsyncs, or a list of {clustername,
%% time_in_minutes} pairs for each sink participating in fullsync
%% replication.
{mapping, "mdc.fullsync_interval.$cluster_name", "riak_repl.fullsync_interval", [
    {datatype, {duration, ms}},
    {include_default, "all"},
    {commented, "30m"}
]}.

{translation,
 "riak_repl.fullsync_interval",
 fun(Conf) ->
    Minute = fun(Millis) -> Millis div 60000 end,
    FullSyncIntervals = cuttlefish_variable:filter_by_prefix("mdc.fullsync_interval", Conf),
    case proplists:get_value(["mdc", "fullsync_interval", "all"], FullSyncIntervals) of
        undefined ->
            [ {list_to_atom(Name), Minute(Value)} || {["mdc", "fullsync_interval", Name], Value} <- FullSyncIntervals];
        X -> Minute(X)
    end
 end}.

%% @doc The maximum size the realtime replication queue can grow to
%% before new objects are dropped. Defaults to 100MB. Dropped objects
%% will need to be replication with a fullsync.
{mapping, "mdc.rtq_max_bytes", "riak_repl.rtq_max_bytes", [
    {datatype, bytesize},
    {default, "100MB"}
]}.

%% @doc Enable Riak CS proxy_get and block filter.
{mapping, "mdc.proxy_get", "riak_repl.proxy_get", [
    {datatype, {enum, [on, off]}},
    {default, off}
]}.

{translation,
 "riak_repl.proxy_get",
 fun(Conf) ->
    case cuttlefish:conf_get("mdc.proxy_get", Conf) of
        on -> enabled;
        off -> disabled;
        _ -> disabled
    end
 end}.

%% @doc A heartbeat message is sent from the source to the sink every
%% heartbeat_interval. Setting heartbeat_interval to undefined
%% disables the realtime heartbeat. This feature is only available in
%% Riak Enterprise 1.3.2+.
{mapping, "mdc.realtime.heartbeat_interval", "riak_repl.rt_heartbeat_interval", [
    {datatype, {duration, s}},
    {default, "15s"}
]}.

%% @doc If a heartbeat response is not received in
%% rt_heartbeat_timeout seconds, then the source connection exits and
%% will be re-established.  This feature is only available in Riak
%% Enterprise 1.3.2+.
{mapping, "mdc.realtime.heartbeat_timeout", "riak_repl.rt_heartbeat_timeout", [
    {datatype, {duration, s}},
    {default, "15s"}
]}.

%% @doc By default, fullsync replication will try to coordinate with other
%% riak subsystems that may be contending for the same resources. This will help
%% to prevent system response degradation under times of heavy load from multiple
%% background tasks. To disable background coordination, set this parameter to false.
%% Enterprise 2.0+.
{mapping, "mdc.fullsync.use_bg_manager", "riak_repl.fullsync_use_background_manager", [
    {datatype, {enum, [true, false]}},
    {level, advanced},
    {default, true}
]}.

%% @doc How frequently the stats for fullsync source processes should be
%% gathered. Requests for fullsync status always returned the most recently
%% gathered data, and thus can be at most as old as this value.
{mapping, "mdc.fullsync.stat_refresh_interval", "riak_repl.fullsync_stat_refresh_interval", [
    {datatype, {duration, ms}},
    {commented, "1m"}
]}.

